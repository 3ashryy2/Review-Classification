{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential , load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Source  ID                                            Message  Target\n",
      "0   Yelp   0                                 Crust is not good.       0\n",
      "1   Yelp   1          Not tasty and the texture was just nasty.       0\n",
      "2   Yelp   2  Stopped by during the late May bank holiday of...       1\n",
      "3   Yelp   3  The selection on the menu was great and so wer...       1\n",
      "4   Yelp   4     Now I am getting angry and I want my damn pho.       0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('sentimentdataset (Project 1).csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "1    1385\n",
      "0    1360\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of samples in each class\n",
    "class_distribution = data['Target'].value_counts()\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Message  Target\n",
      "0                                 Crust is not good.       0\n",
      "1          Not tasty and the texture was just nasty.       0\n",
      "2  Stopped by during the late May bank holiday of...       1\n",
      "3  The selection on the menu was great and so wer...       1\n",
      "4     Now I am getting angry and I want my damn pho.       0\n"
     ]
    }
   ],
   "source": [
    "# Dropping 'Source' and 'ID' columns\n",
    "data = data.drop(['Source', 'ID'], axis=1)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Message  \\\n",
      "0                                 Crust is not good.   \n",
      "1          Not tasty and the texture was just nasty.   \n",
      "2  Stopped by during the late May bank holiday of...   \n",
      "3  The selection on the menu was great and so wer...   \n",
      "4     Now I am getting angry and I want my damn pho.   \n",
      "\n",
      "                                   Processed_Message  \n",
      "0                                       crust good .  \n",
      "1                              tasty texture nasty .  \n",
      "2  stop late bank holiday Rick Steve recommendati...  \n",
      "3                       selection menu great price .  \n",
      "4                          get angry want damn pho .  \n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy's English model\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function for text preprocessing (stop words removal and lemmatization)\n",
    "def preprocess_text(text):\n",
    "    doc = spacy_model(text)\n",
    "    processed_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return processed_text\n",
    "\n",
    "# Apply text preprocessing to the 'Message' column\n",
    "data['Processed_Message'] = data['Message'].apply(preprocess_text)\n",
    "print(data[['Message', 'Processed_Message']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (2196, 4153)\n",
      "Shape of X_test: (549, 4153)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Tf-idf vectorizer\n",
    "tfidf_model = TfidfVectorizer()\n",
    "\n",
    "# Creating feature and target variables\n",
    "X = data['Processed_Message']\n",
    "y = data['Target']\n",
    "\n",
    "# Transform the text data into numerical vectors\n",
    "X_tfidf = tfidf_model.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Experiment (LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1}\n",
      "\n",
      "Classification Report for LinearSVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77       265\n",
      "           1       0.80      0.75      0.77       284\n",
      "\n",
      "    accuracy                           0.77       549\n",
      "   macro avg       0.77      0.77      0.77       549\n",
      "weighted avg       0.77      0.77      0.77       549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize LinearSVC\n",
    "svc_model = LinearSVC(dual=True)\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "hyperparam_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "# Perform Grid Search to find the best parameters\n",
    "grid_search = GridSearchCV(svc_model, hyperparam_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_parameters = grid_search.best_params_\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "\n",
    "# Classification report for LinearSVC\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"\\nClassification Report for LinearSVC:\")\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent Experiment (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.6903 - accuracy: 0.5593\n",
      "Epoch 1: val_loss improved from inf to 0.68538, saving model to best_model.keras\n",
      "62/62 [==============================] - 3s 17ms/step - loss: 0.6899 - accuracy: 0.5617 - val_loss: 0.6854 - val_accuracy: 0.6227\n",
      "Epoch 2/10\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.6484 - accuracy: 0.7399\n",
      "Epoch 2: val_loss improved from 0.68538 to 0.61458, saving model to best_model.keras\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6468 - accuracy: 0.7409 - val_loss: 0.6146 - val_accuracy: 0.7364\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.4906 - accuracy: 0.8376\n",
      "Epoch 3: val_loss improved from 0.61458 to 0.49227, saving model to best_model.keras\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.4906 - accuracy: 0.8376 - val_loss: 0.4923 - val_accuracy: 0.7682\n",
      "Epoch 4/10\n",
      "58/62 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9052\n",
      "Epoch 4: val_loss improved from 0.49227 to 0.45578, saving model to best_model.keras\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 0.2988 - accuracy: 0.9059 - val_loss: 0.4558 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.1995 - accuracy: 0.9306\n",
      "Epoch 5: val_loss improved from 0.45578 to 0.44237, saving model to best_model.keras\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.2008 - accuracy: 0.9297 - val_loss: 0.4424 - val_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9469\n",
      "Epoch 6: val_loss did not improve from 0.44237\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.1488 - accuracy: 0.9469 - val_loss: 0.4561 - val_accuracy: 0.7909\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9600\n",
      "Epoch 7: val_loss did not improve from 0.44237\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.1229 - accuracy: 0.9600 - val_loss: 0.4831 - val_accuracy: 0.8000\n",
      "Epoch 8/10\n",
      "60/62 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9740\n",
      "Epoch 8: val_loss did not improve from 0.44237\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.0911 - accuracy: 0.9742 - val_loss: 0.5025 - val_accuracy: 0.7909\n",
      "Epoch 9/10\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0851 - accuracy: 0.9746\n",
      "Epoch 9: val_loss did not improve from 0.44237\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.0853 - accuracy: 0.9742 - val_loss: 0.5307 - val_accuracy: 0.7909\n",
      "Epoch 10/10\n",
      "59/62 [===========================>..] - ETA: 0s - loss: 0.0674 - accuracy: 0.9825\n",
      "Epoch 10: val_loss did not improve from 0.44237\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.0688 - accuracy: 0.9818 - val_loss: 0.5652 - val_accuracy: 0.7909\n",
      "18/18 [==============================] - 0s 4ms/step\n",
      "\n",
      "Classification Report for ANN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79       265\n",
      "           1       0.82      0.76      0.79       284\n",
      "\n",
      "    accuracy                           0.79       549\n",
      "   macro avg       0.79      0.79      0.79       549\n",
      "weighted avg       0.79      0.79      0.79       549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the ANN model\n",
    "ann_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "ann_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define a callback to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the model with the callback\n",
    "history = ann_model.fit(X_train.toarray(), y_train, epochs=10, batch_size=32, validation_split=0.1, callbacks=[checkpoint], verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_ann = (ann_model.predict(X_test.toarray()) > 0.5).astype(int)\n",
    "\n",
    "# Classification report for ANN\n",
    "print(\"\\nClassification Report for ANN:\")\n",
    "print(classification_report(y_test, y_pred_ann))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the saved best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 101ms/step\n",
      "Sentence: The Burrittos Blah! - Predicted Sentiment: Negative\n",
      "Sentence: The food, amazing. - Predicted Sentiment: Positive\n",
      "Sentence: Service is also cute. - Predicted Sentiment: Positive\n",
      "Sentence: I could care less... The interior is just beautiful. - Predicted Sentiment: Positive\n",
      "Sentence: So they performed. - Predicted Sentiment: Negative\n",
      "Sentence: That's right....the red velvet cake.....ohhh this stuff is so good. - Predicted Sentiment: Positive\n",
      "Sentence: - They never brought a salad we asked for. - Predicted Sentiment: Positive\n",
      "Sentence: This hole in the wall has great Mexican street tacos, and friendly staff. - Predicted Sentiment: Positive\n",
      "Sentence: Took an hour to get our food only 4 tables in restaurant my food was Luke warm, Our server was running around like he was totally overwhelmed. - Predicted Sentiment: Negative\n",
      "Sentence: The worst was the salmon sashimi. - Predicted Sentiment: Positive\n",
      "Sentence: Also there are combos like a burger, fries, and beer for 23 which is a decent deal. - Predicted Sentiment: Positive\n",
      "Sentence: This was like the final blow! - Predicted Sentiment: Negative\n",
      "Sentence: I found this place by accident and I could not be happier. - Predicted Sentiment: Negative\n",
      "Sentence: Overall, I like this place a lot. - Predicted Sentiment: Positive\n",
      "Sentence: The only redeeming quality of the restaurant was that it was very inexpensive. - Predicted Sentiment: Positive\n",
      "Sentence: Ample portions and good prices. - Predicted Sentiment: Positive\n",
      "Sentence: Poor service, the waiter made me feel like I was stupid every time he came to the table. - Predicted Sentiment: Negative\n",
      "Sentence: My first visit to Hiro was a delight! - Predicted Sentiment: Positive\n",
      "Sentence: Service sucks. - Predicted Sentiment: Negative\n",
      "Sentence: The shrimp tender and moist. - Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences as new data\n",
    "new_sentences = [\n",
    "    \"The Burrittos Blah!\",\n",
    "    \"The food, amazing.\",\n",
    "    \"Service is also cute.\",\n",
    "    \"I could care less... The interior is just beautiful.\",\n",
    "    \"So they performed.\",\n",
    "    \"That's right....the red velvet cake.....ohhh this stuff is so good.\",\n",
    "    \"- They never brought a salad we asked for.\",\n",
    "    \"This hole in the wall has great Mexican street tacos, and friendly staff.\",\n",
    "    \"Took an hour to get our food only 4 tables in restaurant my food was Luke warm, Our server was running around like he was totally overwhelmed.\",\n",
    "    \"The worst was the salmon sashimi.\",\n",
    "    \"Also there are combos like a burger, fries, and beer for 23 which is a decent deal.\",\n",
    "    \"This was like the final blow!\",\n",
    "    \"I found this place by accident and I could not be happier.\",\n",
    "    \"Overall, I like this place a lot.\",\n",
    "    \"The only redeeming quality of the restaurant was that it was very inexpensive.\",\n",
    "    \"Ample portions and good prices.\",\n",
    "    \"Poor service, the waiter made me feel like I was stupid every time he came to the table.\",\n",
    "    \"My first visit to Hiro was a delight!\",\n",
    "    \"Service sucks.\",\n",
    "    \"The shrimp tender and moist.\"\n",
    "]\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('best_model.keras')\n",
    "\n",
    "# Define and fit a Tf-idf vectorizer on the existing data and use it to transform new data\n",
    "tfidf_model = TfidfVectorizer()\n",
    "X_tfidf = tfidf_model.fit_transform(data['Processed_Message'])  # 'data' is your original dataset\n",
    "\n",
    "new_sentences_tfidf = tfidf_model.transform(new_sentences)\n",
    "\n",
    "# Make predictions on new data\n",
    "predictions = (loaded_model.predict(new_sentences_tfidf.toarray()) > 0.5).astype(int)\n",
    "\n",
    "# Display predictions positive if 1 negative if 0\n",
    "for sentence, prediction in zip(new_sentences, predictions):\n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    print(f\"Sentence: {sentence} - Predicted Sentiment: {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
